<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>Robotik - Mechanical Eyes</title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Place favicon.ico and apple-touch-icon.png in the root directory -->

        <link rel="stylesheet" href="css/normalize.css">
        <link rel="stylesheet" href="css/bootstrap.min.css">
        <link rel="stylesheet" href="css/pygments.css">
        <link rel="stylesheet" href="css/main.css">
        <link rel="stylesheet" href="css/custom.css">
        <script src="js/vendor/modernizr-2.6.2.min.js"></script>
    </head>
    <body>
        <!--[if lt IE 7]>
            <p class="browsehappy">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
        <![endif]-->

        <!-- Add your site or application content here -->
        <main>
            <section id="landingpage">
                <div>
                    <img src="img/start.jpg" id="start">
                    <a href="#software" class="bullseye" id="bullseye_software" title="Software"></a>
                    <a href="#hardware" class="bullseye" id="bullseye_hardware" title="Hardware"></a>
                    <a href="#video" class="bullseye" id="bullseye_video" title="Video Processing"></a>
                    <a href="#servo" class="bullseye" id="bullseye_servo" title="Servo Steuerung"></a>
                    <a href="#probleme" class="bullseye" id="bullseye_probleme" title="Probleme"></a>
    				<a href="#planung" class="bullseye" id="bullseye_planung" title="Planung"></a>
                    <a href="#team" class="bullseye" id="bullseye_team" title="Das Team"></a>
                    <a href="#downloads" class="bullseye" id="bullseye_downloads" title="Downloads"></a>
                </div>
            </section>

            <section id="software" class="subpage markdown-format">
                <a href="#landingpage" class="btn btn-default btn-lg" role="button" id="back_software">&lt; Zur&uuml;ck</a>
                <h2>Eingesetzte Software</h2>
                <ul class="software_list">
                    <li>Arch Linux ARM f&uuml;r Raspberry Pi (ARMv6) <a href="http://archlinuxarm.org/platforms/armv6/raspberry-pi">http://archlinuxarm.org/platforms/armv6/raspberry-pi</a></li>
                    <li>OpenCV 2.4 <a href="http://opencv.org/">http://opencv.org/</a></li>
                    <li>ServoBlaster <a href="https://github.com/richardghirst/PiBits/tree/master/ServoBlaster">https://github.com/richardghirst/PiBits/tree/master/ServoBlaster</a></li>
                    <li>raspicam 0.0.5 <a href="http://sourceforge.net/projects/raspicam/">http://sourceforge.net/projects/raspicam/</a></li>
                    <li>mechanical eyes (entwickelt im Verlauf dieses Projekts) <a href="https://github.com/bluec0re/mechanical_eyes">https://github.com/bluec0re/mechanical_eyes</a></li>
                </ul>

                <h2>Implementierung</h2>
                Die Implementierung ist aufgeteilt in eine Bibliothek zur Video Analyse, einem Programm zum Testen der Kalibrierung und dem eigentlichen
                Programm welches auf Personen im Bild reagiert und die Servos ansteuert. Die Unterteilung wurde durchgef&uuml;hrt um den Kompilationsaufwand auf dem Raspberry Pi zu minimieren.

                <h3>PersonTracker</h3>
                PersonTracker ist die Bibliothek welche die Bilder von der Raspberry Pi Kamera abruft, je nach gew&auml;hlter Variante
                analysiert (Details im Abschnitt Video Processing). Die Bibliothek liefert die prozentuale Position (Werte zwischen 0 und 1) des gefundenen Gesichtes im Bild.
                Die Konfiguration des Haarcascades Algorithmus erfolgt &uuml;ber die Datei settings.ini.
                <br/>
                PersonTracker liefert ein eigenes Programm (tracker_standalone) mit, welches ein Testen der gew&auml;hlten Paramerter erlaubt.
                Dabei werden 2 Bilder generiert (faces.png und persons.png) welche das Ergebnis der Verarbeitung darstellen.

                <h3>Mechanical Eyes</h3>
                Mechanical Eyes ist das Hauptprogramm welches die Ergebnisse der Gesichtserkennung interpretiert und die Servos
                dementsprechend steuert. Auch hier kann die Feineinstellung der Servos &uuml;ber die Konfigurationsdatei erfolgen.

                <h2>Installationsanleitung</h2>
                <ol>
                    <li>Installiere cmake</li>
                    <li>Installiere opencv</li>
                    <li>Installiere mmal (meistens bereits in der Raspi Firmware enthalten [raspberrypi-firmware-tools])</li>
                    <li>Buildprozess servoblaster
                        <code class="command-line">
                            <span class="command">git clone https://github.com/richardghirst/PiBits</span>
                            <span class="command">cd PiBits/ServoBlaster/user</span>
                            <span class="command">make</span>
                        </code>
                        Optional auf dem raspbian OS (triggert Autostart beim Boot):
                        <code class="command-line">
                            <span class="command">make install</span>
                        </code>
                    </li>
                    <li>Buildprozess raspicam
                        <code class="command-line">
                            <span class="command">cd &lt;mechanical_eyes dir&gt;/libs/raspicam-0.0.5/</span>
                            <span class="command">mkdir build && cd build</span>
                            <span class="command">cmake ..</span>
                            <span class="command">make && make install</span>
                        </code>
                    </li>
                    <li>Buildprozess mechanical_eyes
                        <code class="command-line">
                            <span class="command">cd &lt;mechanical_eyes dir&gt;</span>
                            <span class="command">mkdir build && cd build</span>
                            <span class="command">cmake ..</span>
                            <span class="command">make && make install</span>
                        </code>
                    </li>
                </ol>

                <h2>Konfiguration</h2>
                Die folgende Beispielkonfiguration wird in <i>/etc/mechanical_eyes/settings.ini</i> installiert. Diese
                Konfiguration stellt eine funktionierende Variante mit der aktuellen Hardware Konstellation dar.
                <br>
                Die jeweilige Erkl&auml;hrung f&uuml;r die einzelnen Parameter ist mit Kommentaren versehen.
                <div class="highlight"><pre><span class="c1">; settings for the video recording</span>
<span class="k">[persontracker]</span>
<span class="c1">; image width (default: 640)</span>
<span class="na">width</span> <span class="o">=</span> <span class="s">320</span>
<span class="c1">; image height (default: 480)</span>
<span class="na">height</span> <span class="o">=</span> <span class="s">240</span>
<span class="c1">; brightness value (0-100) (default: 50)</span>
<span class="na">brightness</span> <span class="o">=</span> <span class="s">55</span>
<span class="c1">; contrast value (0-100) (default: 50)</span>
<span class="na">contrast</span> <span class="o">=</span> <span class="s">50</span>
<span class="c1">; saturation value (0-100) (default: 50)</span>
<span class="na">saturation</span> <span class="o">=</span> <span class="s">50</span>
<span class="c1">; gain value (0-100) (default: 50)</span>
<span class="na">gain</span> <span class="o">=</span> <span class="s">50</span>
<span class="c1">; flips the image vertically (default: false)</span>
<span class="na">flip_vertical</span> <span class="o">=</span> <span class="s">true</span>
<span class="c1">; flips the image horizontally (default: false)</span>
<span class="na">flip_horizontal</span> <span class="o">=</span> <span class="s">false</span>

<span class="c1">; settings for the face detection (settings for http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html#cascadeclassifier-detectmultiscale)</span>
<span class="k">[persontracker-face]</span>
<span class="na">scalefactor</span> <span class="o">=</span> <span class="s">1.1</span>
<span class="na">minneighbors</span> <span class="o">=</span> <span class="s">4</span>
<span class="na">flags</span> <span class="o">=</span> <span class="s">6</span>
<span class="na">minsize_x</span> <span class="o">=</span> <span class="s">20</span>
<span class="na">minsize_y</span> <span class="o">=</span> <span class="s">20</span>
<span class="na">maxsize_x</span> <span class="o">=</span> <span class="s">0</span>
<span class="na">maxsize_y</span> <span class="o">=</span> <span class="s">0</span>

<span class="c1">; servo settings</span>
<span class="k">[mechanical_eyes]</span>
<span class="c1">; servo id for vertical control, left side</span>
<span class="na">servo_vertical_left</span> <span class="o">=</span> <span class="s">0</span>
<span class="c1">; minimal allowed servo value</span>
<span class="na">servo_vertical_left_min</span> <span class="o">=</span> <span class="s">110</span>
<span class="c1">; maxium allowed servo value</span>
<span class="na">servo_vertical_left_max</span> <span class="o">=</span> <span class="s">200</span>
<span class="c1">; scaling of the percentage value (max-min)*percentage*scale</span>
<span class="na">servo_vertical_left_scale</span> <span class="o">=</span> <span class="s">-1.0</span>

<span class="c1">; servo id for vertical control, right side (not used at the moment)</span>
<span class="c1">;servo_vertical_right = </span>
<span class="c1">;servo_vertical_right_min = 80</span>
<span class="c1">;servo_vertical_right_max = 130</span>
<span class="c1">;servo_vertical_right_scale = 1.0</span>

<span class="c1">; servo id for horizontal control, left side</span>
<span class="na">servo_horizontal_left</span> <span class="o">=</span> <span class="s">1</span>
<span class="na">servo_horizontal_left_min</span> <span class="o">=</span> <span class="s">80</span>
<span class="na">servo_horizontal_left_max</span> <span class="o">=</span> <span class="s">200</span>
<span class="na">servo_horizontal_left_scale</span> <span class="o">=</span> <span class="s">-1.0</span>

<span class="c1">; servo id for horizontal control, right side</span>
<span class="na">servo_horizontal_right</span> <span class="o">=</span> <span class="s">4</span>
<span class="na">servo_horizontal_right_min</span> <span class="o">=</span> <span class="s">80</span>
<span class="na">servo_horizontal_right_max</span> <span class="o">=</span> <span class="s">200</span>
<span class="na">servo_horizontal_right_scale</span> <span class="o">=</span> <span class="s">-1.0</span>
</pre></div>
                Die Nummerierung der Servos kann &uuml;ber das Device servoblaster-cfg abgerufen werden:

                <code class="command-line">
                    <span class="command">cat /dev/servoblaster-cfg</span>
                </code>
            </section>

            <section id="video" class="subpage">
                <a href="#landingpage" class="btn btn-default btn-lg" role="button" id="back_video">&lt; Zur&uuml;ck</a>
                <h2>Video Processing</h2>
                Zur Erkennung von Personen im aufgezeichneten Bild wird die Grafikbibliothek OpenCV verwendet.
                Dazu wird das Bild zun&auml;chst vorverarbeitet um Fehlerquellen wie Lichtverh&auml;ltnisse m&ouml;glichst
                klein zu halten. Die Vorverarbeitung beinhaltet neben der Umwandlung in ein Schwarz-Wei&szlig;-Bild die
                anschlie&szlig;ende normalisierung der Farbwerte. Dabei wird das Helligkeitsspektrum auf Werte zwischen
                0 und 255 gestreckt. Das resultierende Bild erm&ouml;glicht es leichter Helligkeitsabstufungen zu erkennen.
                Auf ein smoothing zur Rauschunterdr&uuml;ckung wurde explizit verzichtet um die Rechenlast f&uuml;r den
                Raspberry Pi m&ouml;glichst klein zu halten.
                <br/>
                Die eigentliche Erkennung erfolgt mit dem HaarCascade Algorithmus. Dazu werden Helligkeitsdifferenzen
                anhand von zuvor trainierten Modellen im Bild gesucht.
                <div style="text-align: center">
                    <img src="img/haar.png" alt="no-pic" />
                    <div class="caption">Beispiel Erkennnung von Haarcascades (Bild von opencv.org)</div>
                </div>
                <br/>
                Das Resultat bei einem erfolgreich gefundenen Gesicht ist im folgenden Bild zu erkennen:
                <div style="text-align: center">
    				<img src="img/faces.png" alt="no-pic" />
                    <div class="caption">Gesichtserkennung</div>
                </div>
                Ein weiterer Ansatz war die Erkennung von Personen anstelle von Gesichtern. Dazu wurde ein HOG Detektor (<a href="https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients">Wikipedia</a>)
                verwendet. Nach einer erkannten Person wurde die Position des Gesichts nach dem goldenen Schnitt gesch&auuml;tzt.
                Da dieser Ansatz jedoch zu rechenlastig war, wurde dieser im endg&uuml;ltigen Programm nicht mehr verwendet.
            </section>

            <section id="servo" class="subpage">
                <a href="#landingpage" class="btn btn-default btn-lg" role="button" id="back_servo">&lt; Zur&uuml;ck</a>
                <h2>Servo Steuerung</h2>
                Die Servos werden &uuml;ber die GPIO Pins des Raspberry Pi angesteuert. Dazu erfolgt eine Pulsweitenmodulation
                (PWM) durch den Linux Kernel mit Servoblaster als Middleware. Wie im folgenden Bild dargestellt wird die
                Servoposition anhand der Dauer der Signale eingestellt.

                <div style="text-align: center">
                    <img src="img/servosignal.jpg">
                    <div class="caption">PWM Servosignal (<a href="http://www.erkenntnishorizont.de/images/robotik/servosignal.jpg">Quelle</a>)</div>
                </div>

                <h2>Setup</h2>
                Der Anschluss der Servos erfolgt mit einem zus&auml;tzlichen Battery Pack um die Stromlast f&uuml;r den
                Raspberry Pi zu verringern. Das Setup ist f&uuml;r einen Servo beispielhaft im folgenden Bild zu erkennen.


                <div align="center">
                    <img src="img/servo_setup.png">
                    <div class="caption">Servo Setup</div>
                </div>
            </section>

            <section id="hardware" class="subpage">
                <a href="#landingpage" class="btn btn-default btn-lg" role="button" id="back_hardware">&lt; Zur&uuml;ck</a>
				<h2>Verwendete Materialien</h2>
				<ul>
					<li>Aluminium Profil 2,5cm x 1,5cm</li>
					<li>Sperrholzplatten</li>
					<li>Siphonrohr 5cm Durchmesser</li>
					<li>Tischtennisbälle</li>
					<li>Schrauben</li>
					<li>B&uuml;roklammern (Federstahl)</li>
					<li>Raspberry Pi</li>
					<li>Breakout Board</li>
					<li>Modellbau Steckverbindungen</li>
					<li>Batteriepack (inkl. 4 AA Batterien)</li>
				</ul>
				<h2>Aufbau</h2>
				<h3>Rahmen</h3>
				<p>
				Zuerst wurde das 1m lange Aluminiumprofil in insgesamt sechs Teile zersägt. zwei x 20cm und zwei x 5,5cm für ein 
				Rechteck, in welchem später die Augen verbaut werden und zwei x 24cm für zwei Rahmenteile, die mit dem Rechteck 
				links und rechts gelagert verbunden werden, sodass das Rechteck mitsamt den Augen eine Kippbewegung 
				durchführen kann. In die einzelnen Aluminiumteile wurden an 16 verschiedenen Stellen Löcher gebohrt:
				</p>
				<ul>
					<li>2 pro Auge je im Abstand von 7cm vom Rand der langen Rechtecksteile</li>
					<li>8 für das Rechteck (2 pro Teil)</li>
					<li>4 für die Verbindung zwischen Alu-Rechteck und den 2 Rahmenteilen</li>
					<li>4 für die Verbindung zwischen den 2 Rahmenteilen und der Grundplatte</li>
				</ul>
				<p>
				Für die Stabilisierung der Konstruktion wurde eine Grundplatte aus Sperrholz an den Rahmenteilen befestigt.
				</p>
				<h3>Augen</h3>
				<p>
				Für die Augen wurden zwei Tischtennisbälle etwa bei zwei Drittel aufgeschnitten, wobei eine Lasche für eine 
				elastische Verbindung mit den Servos ausgespart wurde. Daraufhin wurden drei Löcher in jeden Ball gebohrt: Zwei 
				an exakt gegenüberliegenden Stellen und eines in die ausgesparte Lasche. Als Indikator für die Blickrichtung 
				der Augen wurde ein Stück graues Klebeband verwendet.
				Um seitlich keinen Einblick in bzw. hinter die Augen zu gewähren wurden 1cm breite Ringe von einem Siphonrohr 
				abgesägt. In diese wurden wiederum auf zwei gegenüberliegenden Seiten Löcher gebohrt, sodass eine Schraube durch 
				das Aluminiumrechteck, den Ring und den Ball durchgesteckt werden konnte. So war es möglich, die horizontale 
				Richtung der Augen separat für jedes Auge und die vertikale für beide Augen zusammen zu steuern.
				</p>
				<h3>Servos</h3>			
				<p>
				Um die Servos auf Höhe der Augen befestigen zu können wurde eine weitere Sperrholzplatte senkrecht auf der 
				Grundplatte festgeschraubt. Die Höhe wurde dabei so bemessen, dass die Servos direkt an die Holzplatte geschraubt 
				werden können und einen idealen Abstand zu den zu bewegenden Teilen besteht. Für die Feinjustage der Servos wurden 
				diese zuerst mit Klebeband auf der senkrecht stehenden Holzplatte fixiert, bevor sie dann mit Spax-Schrauben 
				befestigt wurden.
				</p>
				<div align="center">
					<img src="img/servos.jpg" />
					<div class="caption">Feinjustierung der Servos</div>
				</div>
				<h3>Elektronik</h3>
				<p>
				Um mehr Flexibilität zu gewinnen wurden die benötigten GPIO Anschlüsse des Raspberry Pi Boards mit Steckkabeln auf 
				ein Breakout Board gelegt. So ist es möglich, dass der angeschlossene Akkupack und die Servos einfach miteinander 
				verbunden werden können.
				<div align="center">
					<img src="img/electronics.jpg" />
					<div class="caption">Raspberry Pi mit Breakout Board und Steckverbindungen</div>
				</div>
				</p>
				<h3>Raspberry Pi und RasPiCam</h3>
				<p>
				Das Raspberry Board wurde in einer speziell dafür angefertigten Plastikhülle frontal auf das senkrecht stehende 
				Brett geschraubt. Die Kamera wurde ebenfalls in eine Plastikhülle gepackt und an der Front der Grundplatte 
				zwischen den beiden Rahmenteilen mit Klebenband fixiert, sodass verschiedene Kippwinkel möglich sind - je nachdem 
				wie groß der Höhenunterschied zwischen Roboter und Mensch ist.
				</p>
            </section>

			<section id="team" class="subpage">
                <a href="#landingpage" class="btn btn-default btn-lg" role="button" id="back_team">&lt; Zur&uuml;ck</a>
				<h2>Team</h2>
				<div style="text-align: center">
					<img src="img/team.png" alt="Team" />
                    <div class="caption">Kevin Schaller und Timo Schmid</div>
				</div>
            </section>

			<section id="probleme" class="subpage">
                <a href="#landingpage" class="btn btn-default btn-lg" role="button" id="back_probleme">&lt; Zur&uuml;ck</a>
				<h2>Probleme</h2>

                <h3>Software</h3>
                <ul>
                    <li>
                        Der Raspberry Pi besitzt zwar eine Hardware Decodierung von h264 Videodaten, jedoch reichen sein 800 Mhz BCM2835
                        Prozessor und der 512 MB gro&szlig;e RAM nur begrenzt f&uuml;r gr&ouml;&szlig;ere Bildverarbeitungsprojekte.
                        <ul>
                            <li>Wenig verbesserungspotential au&szlig;er Parametertuning der Algorithmen</li>
                        </ul>
                    </li>
                    <li>
                        Die Software PWM ist nicht echtzeitf&auml;hig, wodurch die gleichzeitige Ansteuerung der Servos
                        beeintr&auml;chtigt ist.
                        <ul>
                            <li>
                                M&ouml;gliche L&ouml;sung k&ouml;nnte der Umstieg auf Hardware PWM oder ein
                                echtzeitf&auml;higes Ger&auml;t sein.
                            </li>
                        </ul>
                    </li>
                </ul>

                <h3>Hardware</h3>
                <ul>
                    <li>
                        Die Raspberry Pi Kamera bietet ohne eine zus&auml;tzliche Linse nur einen begrenzten Field of View
                        (zB. 2.0mx1.33m bei 2m Entfernung und 16:10 Seitenverh&auml;tnis).
                        <ul>
                            <li>
                                Die Verwendung einer Linse f&uuml;r das Kameramodul k&ouml;nnte eine m&ouml;gliche
                                L&ouml;sung darstellen.
                            </li>
                        </ul>
                    </li>
                    <li>
                        Der Raspberry Pi besitzt nur einen Hardware PWM Pin (GPIO 18). Damit musste auf Software PWM
                        ausgewichen werden um mehr als einen Servo anzusprechen.
                        <ul>
                            <li>
                                M&ouml;gliche L&ouml;sung k&ouml;nnte der Anschluss eines gesonderten Boards zur
                                Servosteuerung sein.
                            </li>
                        </ul>
                    </li>
                    <li>
                        Die Servos ben&ouml;tigen mehr Strom als der Raspberry Pi &uuml;ber seine GPIO Pins liefern kann.
                        Dies f&uuml;hrte zu tempor&auml;ren (Raspberry Pi war nicht mehr erreichbar solange die Servos
                        angesteuert wurden) und permanenten (kompletter Absturz des Raspberry Pi) Problemen in der
                        Nutzung
                        <ul>
                            <li>
                                Durch hinzuf&uuml;gen eines zus&auml;tzlichen Batterypacks konnte die Stromzufuhr f&uuml;r
                                die Servos ausgelagert werden.
                            </li>
                        </ul>
                    </li>
                </ul>
            </section>

			<section id="downloads" class="subpage">
                <a href="#landingpage" class="btn btn-default btn-lg" role="button" id="back_downloads">&lt; Zur&uuml;ck</a>
				<h2>Downloads</h2>
				<ul>
					<li><a href="downloads/projektplan_mechanische_augen.pdf">Projektplan</a></li>
					<li><a href="downloads/poster_mechanische_augen.pdf">Poster</a></li>
                    <li><a href="https://github.com/bluec0re/mechanical_eyes">Source code (auf github)</a></li>
				</ul>
            </section>

			<section id="planung" class="subpage">
                <a href="#landingpage" class="btn btn-default btn-lg" role="button" id="back_planung">&lt; Zur&uuml;ck</a>
				<h2>Aufgabenstellung</h2>
				<p>
                Aufgabe war die Planung und Umsetzung eines Roboters, welcher mithilfe eines Raspberry Pis und einer Kamera
                die Gesichter von Personen erkennt und mit per Servo angesteuerten, mechanischen Augen verfolgt. Dabei sollte 
				vor allem darauf geachtet werden, dass der Roboter erweiterbar ist und in einem späteren Projekt zusätzliche 
				Komponenten (z.B. mechanische Augenbrauen) hinzugefügt werden können.
				</p>
				<h2>Planung</h2>
				<h3>Software-Planung</h3>
				<p>
				Da der Raspberry Pi als Plattform vorgegeben war, fiel die Wahl des Betriebssystems recht schnell auf
                                Arch Linux. Vorteile von Arch Linux gegen&uuml;ber dem weit verbreiteten Raspbian (Debian Derivat) sind neben
                                neuster Software auch der minimalistische Ansatz was ein detailiertes Tuning der Laufzeit und hohe Flexibilit&auml;t
                                erm&ouml;glicht.
				</p>
                                <p>
                                F&uuml;r die zweite Anforderung (die Kamera + Bildverarbeitung) fiel die Wahl nach einigen Performance Tests
                                auf das OpenCV Framework. Seine F&uuml;lle an Algorithmen und die Objektorientierung (seit Version 2) ermoeglichte
                                es die Anbindung an das Raspberry Pi Kamera Modul ohne gro&szlig;e Probleme zu realisieren.<br>
                                F&uuml;r die Erkennung wurden verschiedene Algorithmen in Betracht gezogen, darunter Haarcascades und HOG. Da diese
                                schon in optimierter Form in OpenCV enthalten sind, wurden beide Algorithmen in den ersten Softwareprototypen eingebaut.
                                </p>
                                <p>
                                Die Ansteuerung der Servos sollte mithilfe des PWM Modus der GPIO Pins des Raspberry Pi erfolgen. Um die Steuerung zu
                                erleichtern wurde nach einer Bibliothek gesucht welche die Konfiguration abstrahiert. Nach einiger Recherche fiel die
                                Wahl auf das ServoBlaster Modul der PiBits Bibliothek. Da ServoBlaster ein virtuelles Device bereitstellt, bleibt die
                                M&ouml;glichkeit offen die Servos &uuml;ber verschiedenste Programmiersprachen und gegebenenfalls ohne root Rechte
                                anzusteuern.
                                </p>

				<h3>Hardware-Planung</h3>
				<p>
				Da die Augen ein Gesicht nicht nur horizontal sondern auch vertikal verfolgen sollen, wurde im ersten Schritt 
				die Planung der mechanischen Konstruktion der Augen gemacht. Es stellte sich durch Probieren heraus, dass  
				eine Kugel, die auf einen dünnen Stab gesteckt wurde, sich gut für horizontale Drehbewegungen eignet. In einem 
				ersten Prototyp (siehe Abbildung) wurden dafür zwei Tischtennisbälle (4cm Durchmesser) je mit einer 
				Schraube an einem Plastikring (5cm Durchmesser) fixiert. Für die vertikale Bewegung wurden diese Ringe 
				orthogonal zu den Schrauben miteinander verbunden. Auf der jeweilig gegenüberliegenden Seite wurden im Anschluss 
				Schrauben von innen durch die Plastikringe gesteckt, sodass das gesamte Konstrukt drehbar gelagert werden kann.
				Damit die Servos die Augen bewegen können mussten diese in der Höhe der Augen fixiert werden. Einige Versuche 
				ergaben, dass sich Federstahldraht, welcher durch ein Loch auf der Rückseite des Auges mit diesem verbunden wird, 
				gut eignet. Damit kann die horizontale Bewegung durchgeführt werden. Die Vertikale musste dann über ein weiteres 
				Stück Draht an der Oberseite von einem der Ringe realisiert werden.
 				</p>
				<div align="center">
					<img src="img/prototype.jpg" />
					<div class="caption">Erster Prototyp</div>
				</div>
				<p>
				Bei den Versuchen mit dem Prototyp stellte sich heraus, dass die Konstruktion nur dann funktioniert, wenn die 
				Servos für die Horizontale exakt in der Mitte der Augen zwischen dem oberen und unteren Loch fixiert werden, da 
				die vertikale Bewegung sonst die Strecke für das Drahtstück verlängert bzw. verkürtzt.
				</p>
            </section>

            <!-- github ribbon -->
            <a href="https://github.com/bluec0re/mechanical_eyes"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>
        </main>

        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="js/vendor/jquery-1.10.2.min.js"><\/script>')</script>
        <script src="js/bootstrap.min.js"></script>
        <script src="js/plugins.js"></script>
        <script src="js/main.js"></script>
    </body>
</html>
